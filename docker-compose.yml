services:
  dns_updater:
    image: tutkowskim/dns-updater:2026.01.11.0127
    container_name: DnsUpdater
    environment:
      - PORT=8080
      - SENTRY_DSN=${DNS_UPDATER_SENTRY_DSN}
      - AWS_REGION=${DNS_UPDATER_AWS_REGION}
      - AWS_ACCESS_KEY_ID=${DNS_UPDATER_AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${DNS_UPDATER_AWS_SECRET_ACCESS_KEY}
      - HOSTED_ZONE_ID=${DNS_UPDATER_HOSTED_ZONE_ID}
      - RECORD_NAME=${DNS_UPDATER_RECORD_NAME}
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  the_voice:
    image: tutkowskim/the-voice:2026.01.11.0126
    container_name: TheVoice
    environment:
      - SENTRY_DSN=${THE_VOICE_SENTRY_DSN}
      - TOKEN=${THE_VOICE_TOKEN}
      - OPENAI_API_KEY=${THE_VOICE_OPENAI_API_KEY}
      - OPENAI_MODEL=gpt-5.2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4567/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  pihole: # More info at https://github.com/pi-hole/docker-pi-hole/ and https://docs.pi-hole.net/
    container_name: pihole
    image: pihole/pihole:latest
    ports:
      # DNS Ports
      - "53:53/tcp"
      - "53:53/udp"
      # Default HTTP Port
      - "81:80/tcp"
      # Default HTTPs Port. FTL will generate a self-signed certificate
      # - "443:443/tcp"
      # Uncomment the line below if you are using Pi-hole as your DHCP server
      # - "67:67/udp"
      # Uncomment the line below if you are using Pi-hole as your NTP server
      # - "123:123/udp"
    environment:
      # Set the appropriate timezone for your location (https://en.wikipedia.org/wiki/List_of_tz_database_time_zones), e.g:
      TZ: 'America/Los_Angeles'
      # Set a password to access the web interface. Not setting one will result in a random password being assigned
      FTLCONF_webserver_api_password: ${PIHOLE_API_PASSWORD}
      # If using Docker's default `bridge` network setting the dns listening mode should be set to 'ALL'
      FTLCONF_dns_listeningMode: 'ALL'
    # Volumes store your data between container upgrades
    volumes:
      # For persisting Pi-hole's databases and common configuration file
      - './etc-pihole:/etc/pihole'
      # Uncomment the below if you have custom dnsmasq config files that you want to persist. Not needed for most starting fresh with Pi-hole v6. If you're upgrading from v5 you and have used this directory before, you should keep it enabled for the first v6 container start to allow for a complete migration. It can be removed afterwards. Needs environment variable FTLCONF_misc_etc_dnsmasq_d: 'true'
      #- './etc-dnsmasq.d:/etc/dnsmasq.d'
    cap_add:
      # See https://github.com/pi-hole/docker-pi-hole#note-on-capabilities
      # Required if you are using Pi-hole as your DHCP server, else not needed
      - NET_ADMIN
      # Required if you are using Pi-hole as your NTP client to be able to set the host's system time
      - SYS_TIME
      # Optional, if Pi-hole should get some more processing time
      - SYS_NICE
    restart: unless-stopped

  pihole-exporter:
    image: ekofr/pihole-exporter:latest
    container_name: pihole-exporter
    restart: unless-stopped
    environment:
      - PIHOLE_HOSTNAME=pihole
      - PIHOLE_PASSWORD=${PIHOLE_API_PASSWORD}
      - INTERVAL=60s
    ports:
      - "9617:9617"

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    command:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/rootfs
      - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    ports:
      - "9100:9100"

  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: unless-stopped
    ports:
      - "3100:3100"
    command: ["-config.file=/etc/loki/config.yaml"]
    configs:
      - source: loki_config
        target: /etc/loki/config.yaml
    volumes:
      - loki_data:/loki

  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    restart: unless-stopped
    ports:
      - "3200:3200" # Tempo query API
    command: ["/tempo", "-config.file=/etc/tempo/config.yaml"]
    configs:
      - source: tempo_config
        target: /etc/tempo/config.yaml
    volumes:
      - tempo_data:/var/tempo

  alloy:
    image: grafana/alloy:latest
    container_name: alloy
    restart: unless-stopped
    ports:
      - "4317:4317"    # OTLP gRPC (your Java app sends here)
      - "4318:4318"    # OTLP HTTP  (optional)
      - "12345:12345"  # Alloy UI (optional)
    command: ["run", "/etc/alloy/config.alloy"]
    configs:
      - source: alloy_config
        target: /etc/alloy/config.alloy
    depends_on:
      - loki
      - tempo

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    command:
      - /bin/prometheus
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=15d
    configs:
      - source: prometheus_config
        target: /etc/prometheus/prometheus.yml
    volumes:
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    depends_on:
      - alloy

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin # You'll change this on first login
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana

  nginx-proxy:
    image: nginx:alpine
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - "80:80"
    configs:
      - source: nginx_config
        target: /etc/nginx/conf.d/default.conf

configs:
  loki_config:
    content: |
      auth_enabled: false

      server:
        http_listen_port: 3100

      common:
        path_prefix: /loki
        storage:
          filesystem:
            chunks_directory: /loki/chunks
            rules_directory: /loki/rules
        replication_factor: 1
        ring:
          kvstore:
            store: inmemory

      limits_config:
        retention_period: 15d

      compactor:
        working_directory: /loki/compactor
        retention_enabled: true
        delete_request_store: filesystem

      schema_config:
        configs:
          - from: 2024-01-01
            store: tsdb
            object_store: filesystem
            schema: v13
            index:
              prefix: loki_index_
              period: 24h

  tempo_config:
    content: |
      server:
        http_listen_port: 3200

      distributor:
        receivers:
          otlp:
            protocols:
              grpc:
              http:

      storage:
        trace:
          backend: local
          local:
            path: /var/tempo/traces

      compactor:
        compaction:
          block_retention: 360h

  alloy_config:
    content: |
      // Receive OTLP from apps
      otelcol.receiver.otlp "ingest" {
        grpc { endpoint = "0.0.0.0:4317" }
        http { endpoint = "0.0.0.0:4318" }
      }

      otelcol.processor.batch "batch" {}

      // Traces -> Tempo (OTLP gRPC inside the docker network)
      otelcol.exporter.otlp "tempo" {
        client { endpoint = "tempo:4317" }
      }

      // Logs -> Loki
      otelcol.exporter.loki "loki" {
        forward_to = [loki.write.default.receiver]
      }

      loki.write "default" {
        endpoint { url = "http://loki:3100/loki/api/v1/push" }
      }

      // Metrics -> Prometheus scrape endpoint (Alloy exposes :9464)
      otelcol.exporter.prometheus "prom" {
        forward_to = [prometheus.exporter.self.receiver]
      }

      prometheus.exporter "self" {
        listen_address = "0.0.0.0"
        listen_port    = 9464
      }

      // Pipelines
      otelcol.service "pipelines" {
        pipelines {
          traces {
            receivers  = [otelcol.receiver.otlp.ingest]
            processors = [otelcol.processor.batch.batch]
            exporters  = [otelcol.exporter.otlp.tempo]
          }
          logs {
            receivers  = [otelcol.receiver.otlp.ingest]
            processors = [otelcol.processor.batch.batch]
            exporters  = [otelcol.exporter.loki.loki]
          }
          metrics {
            receivers  = [otelcol.receiver.otlp.ingest]
            processors = [otelcol.processor.batch.batch]
            exporters  = [otelcol.exporter.prometheus.prom]
          }
        }
      }

  prometheus_config:
    content: |
      global:
        scrape_interval: 1m
      scrape_configs:
        - job_name: 'alloy'
          static_configs:
            - targets: ['alloy:9464']
        - job_name: 'pihole'
          static_configs:
            - targets: ['pihole-exporter:9617']
        - job_name: 'node'
          static_configs:
            - targets: ['node-exporter:9100']

  nginx_config:
    content: |
      server {
          listen 80;
          server_name portainer.tutkowski.com;
          location / {
              proxy_pass http://192.168.11:9000;
              proxy_http_version 1.1;
              proxy_set_header Upgrade $$http_upgrade;
              proxy_set_header Connection "upgrade";
              proxy_set_header Host $$host;
          }
      }

      server {
          listen 80;
          server_name grafana.tutkowski.com;
          location / {
              proxy_pass http://grafana:3000;
              proxy_set_header Host $$host;
          }
      }

      server {
          listen 80;
          server_name pihole.tutkowski.com;
          location / {
              proxy_pass http://pihole:80;
              proxy_set_header Host $$host;
          }
      }

volumes:
  prometheus_data:
  grafana_data:
  loki_data:
  tempo_data:
